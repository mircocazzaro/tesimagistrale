%!TEX root = ../main.tex

\chapter{System Architecture}
\label{chp:architecture}

As discussed in Chapter \ref{chp:related}, there exist no off-the-shelf solution implementing the \ac{OBDF} paradigm. This implies that for the specific task of dealing with clinical and genomics data, it is necessary to design a system architecture, choosing components in such a way that the whole architecture results in being solid, privacy-oriented (with strong logging capabilities), and being capable to manage genomics data.
This chapter will firstly discuss the overall proposed architecture, that retraces the one presented in previous chapters. Subsequently, each layer will be presented in details, outlining how each components have been configured, reporting eventual code snippets. In order to better describe the source heterogeneity of data that may occur in contexts such as clinical and genomics, available relational data have been distributed among different sources.

\section{System Architecture Design}
\begin{figure}[ht]
    \centering
    \includegraphics[width=15cm]{res/Drawing1.png}
    \caption{Proposed System Architecture}
    \label{fig:mirco_arch}
\end{figure}
The proposed architecture described in Fig. \ref{fig:mirco_arch} implements the \ac{OBDF} framework. It adopts Ontop as its semantic data integration layer and Dremio as its data federation layer. 

Briefly, Ontop have been chosen because it natively supports two high level mapping languages, that gives more freedom on their optimization, without the need to appeal to low level \ac{DL} languages, it is open-source and well maintained by the Free University of Bolzano data integration team, it comes embedded in GraphDB that offers solid \ac{API}s, and it comes even with an embedded web endpoint.

Dremio have been chosen as the virtualization and federation layer because, as discussed in the Chapter \ref{chp:background}, it is an open-source, robust and scalable software influenced both from its enterprise nature and from a consistent community providing contributions. Moreover, within its installation methods, it is possible to set it up through a Docker image: this may set the basis for the architecture packaging, expanding the image to other software components.

The semantic integration requires an ontology well describing the domain of clinical and genomics data. Considering the reusability principle that stands at the basis of the semantic web, we identified the Brainteaser ontology\footnote{https://brainteaser.dei.unipd.it/ontology/} as suitable for accomplishing this task. 

\section{Data Sources}
Given the relational nature of available data, we distributed it among five different platforms, so to exploit the federation capabilities of the data federation layer. The choice of the sources has been guided also by surveying commonly used ones in the biomedicine field in contexts like research and hospital diagnostic. As we will discuss, they encompasses more structured \ac{DBMS}s as well as simple \ac{CSV} files. We also included a novel \ac{DBMS} system, so to investigate how new, unknown data sources can be federated as soon as they figure out.

\subsection{MySQL}
MySQL\footnote{https://www.mysql.com/it/} is one of the most widely used relational \ac{DBMS} (DBMS) in the world. It is a \ac{FOSS} solution now distributed by Oracle\footnote{https://www.oracle.com/it/}. It is extensively employed across a variety of applications, from small personal projects to critical enterprise environments. In our system architecture, we've chosen MySQL considering its extensive adoption, possibly also in application softwares used in clinics and hospitals to collect patients data. 
In our environment, MySQL's role is to store part of the structured clinical data presented in Chapter \ref{chp:context}. 
The interfacing between MySQL and our data federation layer, Dremio, is performed through MySQL's JDBC connector. This setup allows Dremio to access and query MySQL data seamlessly.
No special modifications or configurations were required to integrate MySQL with Dremio, thanks to Dremio's native support for MySQL. We simply established a new data source within Dremio by specifying the connection parameters.

\subsection{PostgreSQL}
PostgreSQL\footnote{https://www.postgresql.org/} stands out as a widely adopted open-source relational \ac{DBMS}. It is particularly adopted within the research community due to its robustness, advanced features, and strong compliance with SQL standards. Many research institutions and academics prefer PostgreSQL for its extensive capabilities in managing complex data types and its support for sophisticated data manipulation operations. We considered PostgreSQL eligible to be a data source due to its common adoption in research contexts as a data collector.
Again, PostgreSQL's role in our environment is to store another portion of the structured clinical data presented in Chapter \ref{chp:context}.
Just like with MySQL, integrating PostgreSQL with Dremio did not require any specific modifications or additional configurations.

\subsection{Polypheny}
Polypheny \cite{DBLP:conf/bigdataconf/VogtSS18} is an open-source polystore system designed to support diverse data models including relational, document, and graph data. It is engineered to handle mixed workloads and various query languages, making it a versatile platform suitable for dynamic data environments.
In our architecture, we consider Polypheny not just as a standalone polystore but as a potential low-level federator under our main data federation layer managed by Dremio. This perspective is particularly useful because it allows us to leverage Polypheny's ability to handle multiple data models, thus enriching the flexibility and capability of our overall data management system.

\subsubsection{Custom ARP Connector Development}
Unlike the straightforward integrations with MySQL and PostgreSQL, incorporating Polypheny required a more sophisticated approach. We performed this task not only to include Polypheny as one of our data sources but also as a proof of concept about the actual possibility of developing custom ARP connectors.
Dremio's Advanced Relational Pushdown (ARP) framework offers a powerful mechanism to extend Dremio's capability to interact with various data sources by intefacing Dremio's internal query representations into the native query language of the target data source.
For Polypheny, we developed a custom ARP connector to ensure interaction between Dremio and Polypheny. The connector we developed is open-source and available for the community, which can be found at our GitHub repository \footnote{https://github.com/mircocazzaro/dremio-polypheny-arp}.
These connectors rely on the target source having a \ac{JDBC} driver and accepting SQL as a query language, so to have an interface to dialog with.

\subsubsection{Connector implementation Details}
The custom ARP connector was implemented to translate SQL queries from Dremio into the query formats that Polypheny can execute directly.
The ARP plugin template\footnote{https://github.com/dremio-hub/dremio-sqllite-connector} consists of a Java Maven project, that has to be compiled, packed within a \ac{JAR} file and injected, together with the target source \ac{JDBC} driver, into the running Dremio instance.
To customize the template, two files needs to be customize: the storage plugin configuration, which is a Java class \ref{lst:polypheny-arp}, and the plugin ARP file, which is a YAML\footnote{https://yaml.org/} file.
The storage plugin configuration file tells Dremio what the name of the plugin should be, what connection options should be displayed in the source UI, what the name of the ARP file is, which JDBC driver to use and how to make a connection to the JDBC driver.
The ARP YAML file is what is used to modify the SQL queries that are sent to the JDBC driver, allowing you to specify support for different data types and functions, as well as rewrite them if tweaks need to be made for your specific data source. 

\begin{lstlisting}[language=JAVA, caption={The storage plugin configuration file}, label={lst:polypheny-arp}]
/* ... */

@SourceType(value = "POLYPHENY", label = "POLYPHENY")
public class PolyphenyConf extends AbstractArpConf<PolyphenyConf> {
  private static final String ARP_FILENAME = "arp/implementation/polypheny-arp.yaml";
  private static final ArpDialect ARP_DIALECT =
      AbstractArpConf.loadArpFile(ARP_FILENAME, (ArpDialect::new));
  private static final String DRIVER = "org.polypheny.jdbc.Driver";

  @NotBlank
  @Tag(1)
  @DisplayMetadata(label = "Polypheny host <HOST:PORT>")
  public String database;

  /* ... */

  @Tag(2)
  @DisplayMetadata(label = "Record fetch size")
  @NotMetadataImpacting
  public int fetchSize = 200;

  @Tag(6)
  @DisplayMetadata(label = "Polypheny User Name")
  public String username = "pa";

  @Tag(7)
  @DisplayMetadata(label = "Polypheny Password")
  public String password = "";

  @Tag(4)
  @DisplayMetadata(label = "Maximum idle connections")
  @NotMetadataImpacting
  public int maxIdleConns = 8;

  @Tag(5)
  @DisplayMetadata(label = "Connection idle time (s)")
  @NotMetadataImpacting
  public int idleTimeSec = 60;

  @VisibleForTesting
  public String toJdbcConnectionString() {
    final String database = checkNotNull(this.database, "Missing database.");

    return String.format("jdbc:polypheny://%s", database);
  }


  private CloseableDataSource newDataSource() {
    return DataSources.newGenericConnectionPoolDataSource(DRIVER,
      toJdbcConnectionString(), username, password, null, DataSources.CommitMode.DRIVER_SPECIFIED_COMMIT_MODE,
            maxIdleConns, idleTimeSec);
  }
}
\end{lstlisting}

With respect to the storage plugin configuration Java class \ref{lst:polypheny-arp}, being Polypheny still an embryonic project not allowing for user management, but rather having a default user "pa" with empty password, fields are pre-filled with default values. The class specification is then interpreted at run time from Dremio, setting up a form with input fields corresponding to each @DisplayMetadata annotation.
The DRIVER static and immutable variable contains the class path of the Polypheny \ac{JDBC} driver. The newDataSource() method is invoked at form submission, setting up the connection to the Polypheny instance.

\subsection{NAS Folders}
Dremio offers native support for a variety of "relational" file types such as \ac{CSV}, Excel, \ac{JSON}, and \ac{VCF}. These files may be physically moved within the Dremio instance, but losing any streaming capability, or by attaching a \ac{NAS} source to it. In fact, Dremio integrates a connector to local folders, reading supported files within them. The name \ac{NAS} on this typology of data source is ambiguous: the term refers to storage units usually located within the same \ac{LAN} of one or more hosts accessing it, while in Dremio is used to generically refer to a folder to which it can access.
This in practice means Dremio can browse local folders: thus, if NAS shared folders are mounted within the Dremio server through the \ac{SMB} protocol, they can be browsed as well.
In Linux environments where the standalone version of Dremio is used, its daemon must have permission to read these folders; in environments where the Docker image is employed, where a clear separation between the host file system and the Docker context occurs, folders and \ac{NAS} shares must be mapped. This is obtained with Docker Volumes\footnote{ https://docs.docker.com/storage/volumes/}, and in particular the “Host Volume” paradigm.
\begin{lstlisting}[language=bash, caption={Docker command to run a Dremio container with a Host Volume}, label={lst:docker-host-volume}]
$ docker run -v NAS_PATH/folder:/opt/dremio/folder -p 9047:9047 -p 31010:31010 -p 45678:45678 -p 32010:32010 --name hereditary_dremio dremio/dremio-oss
\end{lstlisting}
For the purposes of our project, we have utilized the \ac{NAS} data source to store a portion of our genomics data. Specifically, we have chosen to include data in \ac{VCF} files. \ac{VCF} is a text file format generally used in bioinformatics for storing gene sequence variations.

\subsection{Google Drive}
Google Drive is a widely used cloud storage service offered by Google that allows users to save files and access them from any device connected to the internet. Users can store documents, spreadsheets, and presentations, collaborate with others, and have all their work backed up safely.
We chose to consider Google Drive as a data source for our system architecture primarily because of its widespread use in various contexts, including scenarios where Google Sheets are frequently adopted for data collection and management. Google Sheets, part of the Google Drive suite, is particularly popular in both academic and industrial settings for its ease of use and collaborative features.
The approach to integrating Google Drive is identical to that of the \ac{NAS} system, as long as Google Drive is adapted to a local host folder. This adaptation allows Dremio to interact with Google Drive as if it were interacting with local file systems, thereby simplifying access and manipulation of data stored in Google Drive.
The adaptation of Google Drive to a local system is facilitated by a tool known as google-drive-ocamlfuse\footnote{https://github.com/astrada/google-drive-ocamlfuse}, which provides a FUSE-based file system backed by Google Drive. This tool was built using OCaml, a functional programming language known for its expressiveness, efficiency, and robustness. OCaml is utilized to handle the logical operations and data structure management, ensuring that the application runs efficiently and securely. \ac{FUSE} is a software interface for Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is used in google-drive-ocamlfuse to mount Google Drive as a file system on the user's computer.
\subsubsection{Adaptation of Google Sheets}
With the google-drive-ocamlfuse tool, Google Sheets can be adapted to local Excel files, which are natively supported by Dremio. This adaptation is crucial because it allows Dremio to perform operations on Google Sheets just as it would on local Excel files, enabling seamless data processing and integration without needing to convert or move data physically.
\subsubsection{Configuration and Usage}
Configuring google-drive-ocamlfuse involves setting up Google Drive as a mounted file system. Once mounted, the Google Drive folder behaves like any other directory on the local system.
\begin{lstlisting}[language=bash, caption={google-drive-ocamlfuse Tool installation procedure}, label={lst:ocamlfuse}]
# Installation of google-drive-ocamlfuse
$ sudo add-apt-repository ppa:alessandro-strada/ppa
$ sudo apt-get update
$ sudo apt-get install google-drive-ocamlfuse

# Authentication and mounting
$ google-drive-ocamlfuse
$ mkdir ~/google-drive
$ google-drive-ocamlfuse ~/google-drive
\end{lstlisting}
We used the Google Drive data source to store the remaining part of \ac{VCF} genomics data.
